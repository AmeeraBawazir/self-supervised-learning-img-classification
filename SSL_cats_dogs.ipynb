{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SSL_cats_dogs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmeeraBawazir/self-supervised-learning-img-classification/blob/main/SSL_cats_dogs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQO0zxmZWJ1y",
        "outputId": "6beb33a6-44af-4f4b-a9d9-64e485f3545e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "dataset_name = 'cats_vs_dogs'\n",
        "filePath = '/content/gdrive/MyDrive/'\n",
        "(Train, Test), ds_info= tfds.load(name=dataset_name, split=['train[:80%]', 'train[80%:]'], with_info=True, data_dir=filePath)\n",
        "X_train=[]\n",
        "y_train= []\n",
        "for item in Train.take(-1):\n",
        "   image=item[\"image\"]\n",
        "   label=item['label']\n",
        "   image = tf.image.resize(image, (120, 120))\n",
        "   image = image / 255.0\n",
        "   image=image.numpy()\n",
        "   label=label.numpy()\n",
        "   #plt.imshow(image)\n",
        "   #plt.show()\n",
        "   X_train.append(image)\n",
        "   y_train.append(label)\n",
        "X_train=np.array(X_train)\n",
        "y_train=np.array(y_train) \n",
        "\n",
        "X_test=[]\n",
        "y_test= []\n",
        "for item in Test.take(-1):\n",
        "   image=item[\"image\"]\n",
        "   label=item['label']\n",
        "   image = tf.image.resize(image, (120, 120 ))\n",
        "   image = image / 255.0\n",
        "   image=image.numpy()\n",
        "   label=label.numpy()\n",
        "   #plt.imshow(image)\n",
        "   #plt.show()\n",
        "   X_test.append(image)\n",
        "   y_test.append(label)\n",
        "X_test=np.array(X_test)\n",
        "y_test=np.array(y_test)\n",
        "\n",
        "Train=[X_train,y_train]\n",
        "Test=[X_test,y_test]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiIFZYKOXBTb"
      },
      "source": [
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pandas as pd\n",
        "import random\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compliant-helen"
      },
      "source": [
        "def preprocess_data(x_train,y_train,x_test,y_test):\n",
        "        x_train_mean = np.mean(x_train, axis=0)\n",
        "        x_train -= x_train_mean\n",
        "        x_test -= x_train_mean\n",
        "        return x_train,y_train,x_test,y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wound-channels"
      },
      "source": [
        "def rotate_image(im):\n",
        "  iterations = random.randint(0,3)\n",
        "  y = [0,0,0,0]\n",
        "  for i in range(iterations):\n",
        "    im = np.rot90(im)\n",
        "  y[iterations]=1\n",
        "  return im,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3hotR04XwjK"
      },
      "source": [
        "def make_rotated_data(Train,Test):\n",
        "    xy_rot_train = list(zip(*[rotate_image(im) for im in Train[0]]))\n",
        "    xy_rot_test = list(zip(*[rotate_image(im) for im in Test[0]]))\n",
        "    x_rot_train=np.array(xy_rot_train[0][:])\n",
        "    y_rot_train=np.array(xy_rot_train[1][:])\n",
        "    x_rot_test=np.array(xy_rot_test[0][:])\n",
        "    y_rot_test=np.array(xy_rot_test[1][:])\n",
        "    x_rot_train_mean = np.mean(x_rot_train, axis=0)\n",
        "    x_rot_train -= x_rot_train_mean\n",
        "    x_rot_test -= x_rot_train_mean\n",
        "    return x_rot_train,y_rot_train,x_rot_test,y_rot_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhKPeLuYhZLu"
      },
      "source": [
        "## Resnet Implementation taken from David Yang\n",
        "lr = 1e-3\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalizatio:\n",
        "           for res_block in range(num_rn):\n",
        "              x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "def resnet_v1(input_shape, depth, num_classes=10): \n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=7)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86JHtOvGgBFN"
      },
      "source": [
        "def make_ssl_backbone(Train,Test,save_dir,input_shape=(120,120,3),n=3,model_name='cats_dogs.h5'):\n",
        "\n",
        "    x_rot_train,y_rot_train,x_rot_test,y_rot_test = make_rotated_data(Train,Test)\n",
        "    \n",
        "    \n",
        "    # Computed depth from supplied model parameter n\n",
        "    depth = n * 6 + 2\n",
        "    \n",
        "    resnet_model= resnet_v1(input_shape=input_shape, depth=depth)\n",
        "    x = Dense(4,activation='softmax')(resnet_model.layers[-2].output)\n",
        "    model = keras.Model(resnet_model.inputs,x)\n",
        "\n",
        "    filepath = os.path.join(save_dir, model_name)\n",
        "    # Prepare callbacks for model saving and for learning rate adjustment.\n",
        "    checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                                 monitor='val_accuracy',\n",
        "                                 verbose=1,\n",
        "                                 save_best_only=True)\n",
        "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "    lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                                   cooldown=0,\n",
        "                                   patience=10,\n",
        "                                   min_lr=0.5e-6)\n",
        "    callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "    optimizer = keras.optimizers.Adam()\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    history_SSL = model.fit(x_rot_train,y_rot_train,validation_data=(x_rot_test,y_rot_test),epochs=15,callbacks=callbacks)\n",
        "    \n",
        "    history_filepath = save_dir\n",
        "    # convert the history.history dict to a pandas DataFrame:     \n",
        "    hist_df = pd.DataFrame(history_SSL.history) \n",
        "    hist_csv_file = '/content/gdrive/MyDrive/saving/cats_dogs/cats_dogs_ssl.csv'\n",
        "    with open(hist_csv_file, mode='w') as f:\n",
        "        hist_df.to_csv(f)\n",
        "    #model.save(save_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT57LiNchmHL"
      },
      "source": [
        "make_ssl_backbone(Train,Test,save_dir='/content/gdrive/MyDrive/saving/cats_dogs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ6e2hApOE9d"
      },
      "source": [
        "def train_models_datasplit(X_train,y_train,X_test,y_test,\n",
        "                               batch_size = 32,epochs=12,data_augmentation=True,\n",
        "                               num_classes=2,\n",
        "                               n = 3,ssl_path=None,dataset='cats_dogs',input_shape=(120,120,3),\n",
        "                           data_fraction_increment=0.1,num_increments=10,random_state=689):\n",
        "     \n",
        "    ###Calcualte number of many kfolds splits \n",
        "    n_splits=int(1/data_fraction_increment)\n",
        "        \n",
        "        \n",
        "    # Convert class vectors to binary class matrices.\n",
        "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "    # Computed depth from supplied model parameter n\n",
        "    depth = n * 6 + 2\n",
        "    # Model name, depth and version\n",
        "    model_type = 'ResNet%d' % depth\n",
        "    \n",
        "    ###Calcualte number of many kfolds splits \n",
        "    n_splits=int(1/data_fraction_increment)\n",
        "    \n",
        "    ##Ensure that data increments consist of class balanced data\n",
        "    skf = StratifiedKFold(n_splits=n_splits,random_state=random_state)\n",
        "    \n",
        "    histories = []\n",
        "    for i,index  in enumerate(skf.split(X_train,y=y_train)):\n",
        "        if i>num_increments-1:\n",
        "            break\n",
        "        if i > 0:\n",
        "            test_index = np.concatenate((test_index,index[1]))\n",
        "        else:\n",
        "\n",
        "          332\n",
        "            test_index=index[1]\n",
        "        y_train_t = keras.utils.to_categorical(y_train, num_classes)[test_index]\n",
        "        if ssl_path:\n",
        "            ssl_model=keras.models.load_model(ssl_path)\n",
        "            x=model=ssl_model.layers[-2].output\n",
        "            x = Dense(2,activation='softmax')(x)\n",
        "            model = keras.Model(ssl_model.inputs,x)\n",
        "        else:\n",
        "            model = resnet_v1(input_shape=input_shape, depth=depth,num_classes=2)\n",
        "        \n",
        "        model.compile(loss='binary_crossentropy',\n",
        "                      optimizer=Adam(learning_rate=lr_schedule(0)),\n",
        "                      metrics=['accuracy'])\n",
        "        # Prepare model model saving directory.\n",
        "        save_dir = os.path.join('/content/gdrive/MyDrive/saving/cats_dogs')\n",
        "        model_name = dataset +\"_\"+ str((i/n_splits)*100)+\".h5\"\n",
        "        if not os.path.isdir(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        filepath = os.path.join(save_dir, model_name)\n",
        "        #filepath=os.path.join(filepath,\".h5\")\n",
        "        \n",
        "        # Prepare callbacks for model saving and for learning rate adjustment.\n",
        "        checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                                      monitor='val_loss',\n",
        "                                      verbose=1,\n",
        "                                      save_best_only=True)\n",
        "        lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "        lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                                       cooldown=0,\n",
        "                                       patience=5,\n",
        "                                       min_lr=0.5e-6)\n",
        "        callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "        \n",
        "        # Run training, with or without data augmentation.\n",
        "        if not data_augmentation:\n",
        "            model.fit(X_train, y_train,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs,\n",
        "                      validation_data=(x_test, y_test),\n",
        "                      shuffle=True,\n",
        "                      callbacks=callbacks)\n",
        "        else:\n",
        "            # This will do preprocessing and realtime data augmentation:\n",
        "            \n",
        "            histories.append(model.fit(X_train[test_index], y_train_t,batch_size=batch_size,\n",
        "                                epochs=epochs, verbose=1,validation_data=(X_test,y_test),\n",
        "                                callbacks=callbacks))\n",
        "        # Record performance of trained model.\n",
        "        train_maxes = []\n",
        "        val_maxes = []\n",
        "        val_loss = []\n",
        "        train_loss = []\n",
        "        if ssl_path:\n",
        "            record_path = '/content/gdrive/MyDrive/saving/cats_dogs/' +dataset+'_self_supervised.csv'\n",
        "        else:\n",
        "            record_path = '/content/gdrive/MyDrive/saving/cats_dogs/' +dataset+'_supervised.csv'\n",
        "\n",
        "        for i in range(len(histories)):\n",
        "            val_maxes.append(max(histories[i].history['val_accuracy']))\n",
        "            train_maxes.append(max(histories[i].history['accuracy']))\n",
        "            val_loss.append(min(histories[i].history['val_loss']))\n",
        "            train_loss.append(min(histories[i].history['loss']))\n",
        "        pd.DataFrame({'Val_Accuracy':val_maxes,\n",
        "                      'Train_Accuracy':train_maxes,\n",
        "                      'Val_Loss':val_loss,'Train_Loss':train_loss}).to_csv(record_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRlYr4O6OE9f"
      },
      "source": [
        "### Run experiment WITHOUT self-supervision\n",
        "x_train, y_train, x_test, y_test = preprocess_data(X_train, y_train, X_test, y_test)\n",
        "train_models_datasplit(x_train, y_train, x_test, y_test,batch_size=32,num_increments=10,dataset='cats_dogs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNHBncpDla87"
      },
      "source": [
        "### Run experiment with WITH self-supervision\n",
        "x_train, y_train, x_test, y_test = preprocess_data(x_train, y_train, x_test, y_test)\n",
        "train_models_datasplit(x_train, y_train, x_test, y_test,batch_size=32,num_increments=10,dataset='cats_dogs_ssl',ssl_path='/content/gdrive/MyDrive/saving/cats_dogs/cats_dogs.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIGnWn_VM-Qp"
      },
      "source": [
        "#Visualization for downstream Accuracy and Loss \n",
        "df_sl = pd.read_csv('cats_vs_dogs_supervised.csv')\n",
        "df_ssl = pd.read_csv('cats_vs_dogs_self_supervised.csv')\n",
        "df_sl['Model_Type']='Supervised'\n",
        "df = pd.concat((df_sl,df_ssl))\n",
        "df['Model_Type'][len(df_ssl):]='Self_Supervised'\n",
        "df['Number_of_labeled_data'] = (df['Unnamed: 0']+1) *18610*0.1 #18610 is size of train set, 0.1 is data_fraction_increment\n",
        "\n",
        "\n",
        "figure = plt.figure(figsize=(15,7))\n",
        "g = sns.lineplot(y='Val_Accuracy',x='Number_of_labeled_data',hue='Model_Type',data=df)\n",
        "g.set(xticks=df['Number_of_labeled_data'].values)\n",
        "g.set_ylim(0.7,1)\n",
        "title='Accuracy Self-Supervised vs Only Supervised'\n",
        "g.set_title(title, fontsize=25)\n",
        "g.set_xlabel(\"Number of training data\",fontsize=25) #x label + font size \n",
        "g.set_ylabel(\"Validation accuracy\",fontsize=25)#y label + font size \n",
        "g.tick_params(labelsize=25)#size of num in x and y axis \n",
        "\n",
        "\n",
        "figure = plt.figure(figsize=(15,7))\n",
        "f = sns.lineplot(y='Val_Loss',x='Number_of_labeled_data',hue='Model_Type',data=df)\n",
        "f.set(xticks=df['Number_of_labeled_data'].values)\n",
        "title='Loss Self-Supervised vs Only Supervised'\n",
        "f.set_title(title, fontsize=25)\n",
        "f.set_xlabel(\"Number of training data\",fontsize=25)\n",
        "f.set_ylabel(\"Validation loss\",fontsize=25)\n",
        "f.set_ylim(0,1)\n",
        "f.tick_params(labelsize=25)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEWPd_avOIUR"
      },
      "source": [
        "##visualize ssl model  filters by loading the trained model \n",
        "import matplotlib\n",
        "from matplotlib import pyplot\n",
        "\n",
        "save_dir = os.getcwd()\n",
        "model_name='Restnetv1_SL_.h5' \n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "#trying to load the saved model in oredr to visualize the filter \n",
        "model=keras.models.load_model(filepath)\n",
        "model.summary() \n",
        "tf.keras.utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "## summarize filter shapes\n",
        "for layer in model.layers:\n",
        "    # check for convolutional layer\n",
        "    if 'conv' not in layer.name:\n",
        "        continue\n",
        "    #get filter weights\n",
        "    filters, biases = layer.get_weights()\n",
        "    print(layer.name, filters.shape)\n",
        "    \n",
        "#retrieve weights from the second hidden layer\n",
        "filters, biases = model.layers[1].get_weights()\n",
        "filters.shape #in first layer: 16 blocks of 3 matrix each is 3x3 (3,3,3,16)\n",
        "\n",
        "\n",
        "\n",
        "# normalize filter values to 0-1 so we can visualize them\n",
        "f_min, f_max = filters.min(), filters.max()\n",
        "filters = (filters - f_min) / (f_max - f_min)\n",
        "filters\n",
        "\n",
        "# plot first few filters\n",
        "n_filters, ix = 16, 1 #determins how many filters to plot\n",
        "for i in range(n_filters):\n",
        "    # get the filter\n",
        "    f = filters[:, :, :, i]\n",
        "    # plot each channel separately\n",
        "    for j in range(3): \n",
        "        # specify subplot and turn of axis\n",
        "        ax = pyplot.subplot(n_filters, 3, ix)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        # plot filter channel in grayscale\n",
        "        pyplot.imshow(f[:, :, j], cmap='gray')\n",
        "        ix += 1\n",
        "# show the figure\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzgxwx14OeWQ"
      },
      "source": [
        "#visualize feature maps\n",
        "save_dir = os.getcwd()\n",
        "model_name='Restnetv1_SL_.h5'\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "model=keras.models.load_model(filepath)\n",
        "# summarize feature map shapes\n",
        "for i in range(len(model.layers)):\n",
        "\tlayer = model.layers[i]\n",
        "\t# check for convolutional layer\n",
        "\tif 'conv' not in layer.name:\n",
        "\t\tcontinue\n",
        "\t# summarize output shape\n",
        "\tprint(i, layer.name, layer.output.shape)\n",
        "# redefine model to output right after the first hidden layer\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[1].output)\n",
        "#select one image from train data \n",
        "img = Train[0][0]\n",
        "# expand dimensions so that it represents a single 'sample'\n",
        "img = np.expand_dims(img, axis=0)\n",
        "feature_maps = model.predict(img)\n",
        "\n",
        "\n",
        "# plot all 16 maps in an 4x4 squares\n",
        "square = 4\n",
        "ix = 1\n",
        "for _ in range(square):\n",
        "\tfor _ in range(square):\n",
        "\t\t# specify subplot and turn of axis\n",
        "\t\tax = pyplot.subplot(square, square, ix)\n",
        "\t\tax.set_xticks([])\n",
        "\t\tax.set_yticks([])\n",
        "\t\t# plot filter channel in grayscale\n",
        "\t\tpyplot.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
        "\t\tix += 1\n",
        "# show the figure\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}